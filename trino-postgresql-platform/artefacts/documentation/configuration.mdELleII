# `modern-data-platform` - Configuration v1.17.0

This is the documentation of the configuration settings which can be overwritten using a custom YAML file. All the defaults are defined in [`../modern-data-platform-stack/generator-config/vars/config.yml`](../modern-data-platform-stack/generator-config/vars/config.yml).

## Overall Settings

There are some overall settings which will control the behaviour for all or a group of services. These are listed in the table below.

| Config                                         	| Default 	| Since 	| Description                                                                                                                                                                        	                    	|
|------------------------------------------------	|:-------:	|-------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| `use_timezone`                             	|  	| 1.5.0 | The timezone to use for the whole stack. By default is empty so the timezone of the docker engine is not changed and it will run as `Etc/UTC`. If you want to set it to another timezone, then specify a Unix timezone string, such as `Europe/Zurich` or `America/New_York`. An overview on the valid timezones can be found here: <https://en.wikipedia.org/wiki/List_of_tz_database_time_zones> |  
| `private_docker_repository_name`                             	| `trivadis` 	| 1.5.0 | Docker images not available on public Docker Hub will be retrieved using this private repository. By default it points to `trivadis` and you have to login first, before you can use the generated stack, if you have selected a private image. Use this config to point to your own private docker registry if needed. |  
| `uid`                             	| `1000` 	| 1.9.0 | The UID to use when using the "user" property in a service to override the user inside the container. |  
| `env`           | ${PLATYS_ENV} 	| 1.16.0 | Optional environment identifier of this platys instance, by default take it from the environment variable (can be specified in the `.env` file), but can be changed to hardcoded value. Allowed values (taken from [DataHub](https://datahubproject.io/docs/graphql/enums/#fabrictype)): `dev`, `test`, `qa`, `uat`, `ei`, `pre`, `non_prod`, `prod`, `corp` |  
| `data_centers`                             	| `dc1,dc2` 	| 1.14.0 | A comma-separated list of data-center names, to use if the property `data_center_to_use` has a value != 0. |  
| `data_center_to_use`                             	| `0` 	| 1.14.0 | The data-center to use, if multiple DC should be simulated for a Kafka setup. |  
| `copy_cookbook_data_folder`                             	| `true` 	| 1.14.0 | Copy all the `data` folders of the various cookbook recipes into the `data-transfer/cookbook-data` folder. |  

## External Services

There are some overall settings which will control the behaviour for all or a group of services. These are listed in the table below.

| Config                                         	| Default 	| Since 	| Description                                                                                                                                                                        	                    	|
|------------------------------------------------	|:-------:	|-------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| `KAFKA_enable`                             	| `false` | 1.9.0 | Use external Kafka service, such as Confluent Cloud. Specify the cluster through the `KAFKA_bootstrap_servers` property.  |  
| `KAFKA_bootstrap_servers`                             	| `` 	| 1.9.0 | A comma-separated list of host and port pairs that addresses the external Kafka brokers |  
| `KAFKA_username`                             	| `` 	| 1.9.0 | Username to authenticate against the external Kafka cluster |  
| `KAFKA_password`                             	| `` 	| 1.9.0 | Password to authenticate against the external Kafka cluster |  
| `SCHEMA_REGISTRY_enable`                             	| `false` 	| 1.9.0 |  Use an external schema registry |  
| `SCHEMA_REGISTRY_url`                             	| `` 	| 1.9.0 | The URL of the external schema registry |  
| `S3_enable`                             	| `false` | 1.9.0 | Use external S3 service, such as AWS S3 cloud service or an on-premises S3 appliance. You have to configure two environment variables, `PLATYS_AWS_ACCESS_KEY` with the access key and `PLATYS_AWS_SECRET_ACCESS_KEY` with the access secret. This can be done on the on the docker host or in the `.env` file in the platform home (same folder where the `docker-compose.yml` is located). |  
| `S3_endpoint`                             	| `` | 1.9.0 | The endpoint address of the S3 external service |  
| `S3_path_style_access`                             	| `false` | 1.9.0 | Use Path Style Access if set to `true`, otherwise the default of virtual hosted-style access is used. |
| `ADLS_enable`                             	| `false` | 1.15.0 | Use external Azure Data Lake Storage Gen2 service. You have to configure two environment variables, `PLATYS_AZURE_ADLS_ACCESS_KEY` with the access key. This can be done on the on the docker host or in the `.env` file in the platform home (same folder where the `docker-compose.yml` is located). |  
| `ADLS_storage_account`                             	| `` | 1.15.0 | The name of the storage account for the ADLS service. |  
| `DATAHUB_enable`                             	| `false` | 1.16.0 | Use external DataHub service. Specify the DataHub GMS service through the `DATAHUB_gms_url` property.  |  
| `DATAHUB_gms_url`                             	| `` 	| 1.16.0 | the web url of the external DataHub GMS service instance to connect to. |  

## Platform Services

The configuration settings for enabling/disabling a given service are named `XXXXX_enable` where XXXXX is the name of the service (he used to be named `XXXXX_enabled` in version 1.0.0).
For each service there might be some other settings, such as controlling the number of nodes to start the service with, whether the service should map a data volume into the container or controlling some other proprietary configuration properties of the service itself.

### Stream Processing Ecosystem

| Config                                         	| Default 	| Since 	| Description                                                                                                                                                                        	                    	|
|------------------------------------------------	|:-------:	|-------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| [**_Apache Zookeeper_**](./services/zookeeper)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                	|         	|       	|                                                                                                                                                                                     	|
| `ZOOKEEPER_enable`                             	| `false` 	| 1.0.0 	| [Apache Zookeeper](https://zookeeper.apache.org/) is a coordination service used by Apache Kafka and Apache Atlas services. It is automatically enabled if using either of the two. 	|             	|                                
| `ZOOKEEPER_nodes`                              	|   `1`   	| 1.0.0 	| number of Zookeeper nodes                                                                                                                                                           	|
| `ZOOKEEPER_navigator_enable`                   	| `false` 	| 1.1.0 	| Zookeeper Navigator is a UI for managing and viewing zookeeper cluster.                                                                                                             	|      
| [**_Apache Kafka_**](./services/kafka)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                    	|         	|       	|                                                                                                                                                                                     	|
| `KAFKA_enable`                                 	| `false` 	| 1.0.0 	| Use confluent Kafka                                                                                                                                                                 	|             	|                                	|
| `KAFKA_edition`                      	| `community` 	| 1.2.0 	| The Kafka edition to use, one of `community` or `enterprise`                                                                                                                                               	|             	|                                	|
| `KAFKA_volume_map_data`                        	| `false` 	| 1.0.0 	| Volume map data folder into the Kafka broker                                                                                                                                        	|             	|                                	|
| `KAFKA_use_standard_port_for_external_interface`                        	| `true` 	| 1.14.0 	| Should the standard Port 9092 - 9095 to be used for the external interface or for the private interface (docker host)?                                                                                                                                      	|             	|                                	|
| `KAFKA_nodes`                                  	|   `3`   	| 1.0.0 	| number of Kafka Broker nodes to use                                                                                                                                                 	|             	|                                	|
| `KAFKA_use_kraft_mode`                                  	|   `false`   	| 1.13.0 	| use Zookeeper-Less setup of Kafka 2.8.0 (Confluent 6.2) available as a preview.                                                                                                                                                 	|             	|                                	|
| `KAFKA_internal_replication_factor`                    	| `3` 	| 1.6.0 	| the replication factor to use for the Kafka internal topics                                                                                                                                                      	|             	|                                	|
| `KAFKA_delete_topic_enable`                    	| `true` 	| 1.0.0 	| allow deletion of Kafka topics                                                                                                                                                      	|             	|                                	|
| `KAFKA_auto_create_topics_enable`              	| `false` 	| 1.0.0 	| allow automatic creation of Kafka topics                                                                                                                                            	|             	|                                	|
| `KAFKA_message_timestamp_type`              	| `CreateTime` 	| 1.8.0 	| Define whether the timestamp in the message is message create time or log append time. The value should be either `CreateTime` or `LogAppendTime`.                                                                                                                                            	|             	|                                	|
| `KAFKA_jmx_monitoring_prometheus_enable`              	| `false` 	| 1.12.0 	| Enable Kafka JMX Monitoring over Prometheus. If enabled, then `prometheus` and `grafana` are automatically enabled as well.                                                                                                                                            	|             	|                                	|
| `KAFKA_log_segment_bytes`              	| `1073741824` (1 GB) | 1.8.0 	| The maximum size of a single log file.                                                                                                                                            	|             	
| `KAFKA_log_retention_ms`              	| `` | 1.8.0 	| The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in `log.retention.hours` is used. If set to `-1`, no time limit is applied.                                                                                                                                            	|
| `KAFKA_log_retention_hours`              	| `168` (1 GB) | 1.8.0 	| The number of hours to keep a log file before deleting it (in hours), tertiary to `log.retention.ms` property                                                                                                                                            	|
| `KAFKA_log_retention_bytes`              	| `-1` (not used) | 1.8.0 	| The maximum size of the log before deleting it.                                                                                                                                            	|
| `KAFKA_compression_type`              	| `producer` | 1.8.0 	| Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `uncompressed` which is equivalent to no compression; and `producer` which means retain the original compression codec set by the producer.                                                                                                                                            	|
| `KAFKA_min_insync_replicas`              	| `1`  | 1.8.0 	| When a producer sets acks to "all" (or "-1"), `min.insync.replicas` specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).                                                                                                                                  	|             	
| `KAFKA_replica_selector_class`              	|  	| 1.8.0 	| The fully qualified class name that implements ReplicaSelector. This is used by the broker to find the preferred read replica. By default, an implementation that returns the leader.                                                                                                                                            	|             	|                                	|
| `KAFKA_confluent_log_placement_constraints`              	| `{}` 	| 1.8.0 	| This configuration is a JSON object that controls the set of brokers (replicas) which will always be allowed to join the ISR. And the set of brokers (observers) which are not allowed to join the ISR. Only enabled if `KAFKA_edition` is set to `enterprise`. Find more information here: <https://docs.confluent.io/current/installation/configuration/broker-configs.html>.                                                                                                                                             	|             	|                                	|