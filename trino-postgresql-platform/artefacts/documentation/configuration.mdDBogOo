# `modern-data-platform` - Configuration v1.16.0

This is the documentation of the configuration settings which can be overwritten using a custom YAML file. All the defaults are defined in [`../modern-data-platform-stack/generator-config/vars/config.yml`](../modern-data-platform-stack/generator-config/vars/config.yml).

## Overall Settings

There are some overall settings which will control the behaviour for all or a group of services. These are listed in the table below.

| Config                                         	| Default 	| Since 	| Description                                                                                                                                                                        	                    	|
|------------------------------------------------	|:-------:	|-------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| `use_timezone`                             	|  	| 1.5.0 | The timezone to use for the whole stack. By default is empty so the timezone of the docker engine is not changed and it will run as `Etc/UTC`. If you want to set it to another timezone, then specify a Unix timezone string, such as `Europe/Zurich` or `America/New_York`. An overview on the valid timezones can be found here: <https://en.wikipedia.org/wiki/List_of_tz_database_time_zones> |  
| `private_docker_repository_name`                             	| `trivadis` 	| 1.5.0 | Docker images not available on public Docker Hub will be retrieved using this private repository. By default it points to `trivadis` and you have to login first, before you can use the generated stack, if you have selected a private image. Use this config to point to your own private docker registry if needed. |  
| `uid`                             	| `1000` 	| 1.9.0 | The UID to use when using the "user" property in a service to override the user inside the container. |  
| `data_centers`                             	| `dc1,dc2` 	| 1.14.0 | A comma-separated list of data-center names, to use if the property `data_center_to_use` has a value != 0. |  
| `data_center_to_use`                             	| `0` 	| 1.14.0 | The data-center to use, if multiple DC should be simulated for a Kafka setup. |  
| `copy_cookbook_data_folder`                             	| `true` 	| 1.14.0 | Copy all the `data` folders of the various cookbook recipes into the `data-transfer/cookbook-data` folder. |  

## External Services

There are some overall settings which will control the behaviour for all or a group of services. These are listed in the table below.

| Config                                         	| Default 	| Since 	| Description                                                                                                                                                                        	                    	|
|------------------------------------------------	|:-------:	|-------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| `KAFKA_enable`                             	| `false` | 1.9.0 | Use external Kafka service, such as Confluent Cloud. Specify the cluster through the `KAFKA_bootstrap_servers` property.  |  
| `KAFKA_bootstrap_servers`                             	| `` 	| 1.9.0 | A comma-separated list of host and port pairs that addresses the external Kafka brokers |  
| `KAFKA_username`                             	| `` 	| 1.9.0 | Username to authenticate against the external Kafka cluster |  
| `KAFKA_password`                             	| `` 	| 1.9.0 | Password to authenticate against the external Kafka cluster |  
| `SCHEMA_REGISTRY_enable`                             	| `false` 	| 1.9.0 |  Use an external schema registry |  
| `SCHEMA_REGISTRY_url`                             	| `` 	| 1.9.0 | The URL of the external schema registry |  
| `S3_enable`                             	| `false` | 1.9.0 | Use external S3 service, such as AWS S3 cloud service or an on-premises S3 appliance. You have to configure two environment variables, `PLATYS_AWS_ACCESS_KEY` with the access key and `PLATYS_AWS_SECRET_ACCESS_KEY` with the access secret. This can be done on the on the docker host or in the `.env` file in the platform home (same folder where the `docker-compose.yml` is located). |  
| `S3_endpoint`                             	| `` | 1.9.0 | The endpoint address of the S3 external service |  
| `S3_path_style_access`                             	| `false` | 1.9.0 | Use Path Style Access if set to `true`, otherwise the default of virtual hosted-style access is used. |
| `ADLS_enable`                             	| `false` | 1.15.0 | Use external Azure Data Lake Storage Gen2 service. You have to configure two environment variables, `PLATYS_AZURE_ADLS_ACCESS_KEY` with the access key. This can be done on the on the docker host or in the `.env` file in the platform home (same folder where the `docker-compose.yml` is located). |  
| `ADLS_storage_account`                             	| `` | 1.15.0 | The name of the storage account for the ADLS service. |  

## Platform Services

The configuration settings for enabling/disabling a given service are named `XXXXX_enable` where XXXXX is the name of the service (he used to be named `XXXXX_enabled` in version 1.0.0).
For each service there might be some other settings, such as controlling the number of nodes to start the service with, whether the service should map a data volume into the container or controlling some other proprietary configuration properties of the service itself.

### Stream Processing Ecosystem

| Config                                         	| Default 	| Since 	| Description                                                                                                                                                                        	                    	|
|------------------------------------------------	|:-------:	|-------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| [**_Apache Zookeeper_**](./services/zookeeper)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                	|         	|       	|                                                                                                                                                                                     	|
| `ZOOKEEPER_enable`                             	| `false` 	| 1.0.0 	| [Apache Zookeeper](https://zookeeper.apache.org/) is a coordination service used by Apache Kafka and Apache Atlas services. It is automatically enabled if using either of the two. 	|             	|                                
| `ZOOKEEPER_nodes`                              	|   `1`   	| 1.0.0 	| number of Zookeeper nodes                                                                                                                                                           	|
| `ZOOKEEPER_navigator_enable`                   	| `false` 	| 1.1.0 	| Zookeeper Navigator is a UI for managing and viewing zookeeper cluster.                                                                                                             	|      
| [**_Apache Kafka_**](./services/kafka)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                    	|         	|       	|                                                                                                                                                                                     	|
| `KAFKA_enable`                                 	| `false` 	| 1.0.0 	| Use confluent Kafka                                                                                                                                                                 	|             	|                                	|
| `KAFKA_edition`                      	| `community` 	| 1.2.0 	| The Kafka edition to use, one of `community` or `enterprise`                                                                                                                                               	|             	|                                	|
| `KAFKA_volume_map_data`                        	| `false` 	| 1.0.0 	| Volume map data folder into the Kafka broker                                                                                                                                        	|             	|                                	|
| `KAFKA_use_standard_port_for_external_interface`                        	| `true` 	| 1.14.0 	| Should the standard Port 9092 - 9095 to be used for the external interface or for the private interface (docker host)?                                                                                                                                      	|             	|                                	|
| `KAFKA_nodes`                                  	|   `3`   	| 1.0.0 	| number of Kafka Broker nodes to use                                                                                                                                                 	|             	|                                	|
| `KAFKA_use_kraft_mode`                                  	|   `false`   	| 1.13.0 	| use Zookeeper-Less setup of Kafka 2.8.0 (Confluent 6.2) available as a preview.                                                                                                                                                 	|             	|                                	|
| `KAFKA_internal_replication_factor`                    	| `3` 	| 1.6.0 	| the replication factor to use for the Kafka internal topics                                                                                                                                                      	|             	|                                	|
| `KAFKA_delete_topic_enable`                    	| `true` 	| 1.0.0 	| allow deletion of Kafka topics                                                                                                                                                      	|             	|                                	|
| `KAFKA_auto_create_topics_enable`              	| `false` 	| 1.0.0 	| allow automatic creation of Kafka topics                                                                                                                                            	|             	|                                	|
| `KAFKA_message_timestamp_type`              	| `CreateTime` 	| 1.8.0 	| Define whether the timestamp in the message is message create time or log append time. The value should be either `CreateTime` or `LogAppendTime`.                                                                                                                                            	|             	|                                	|
| `KAFKA_jmx_monitoring_prometheus_enable`              	| `false` 	| 1.12.0 	| Enable Kafka JMX Monitoring over Prometheus. If enabled, then `prometheus` and `grafana` are automatically enabled as well.                                                                                                                                            	|             	|                                	|
| `KAFKA_log_segment_bytes`              	| `1073741824` (1 GB) | 1.8.0 	| The maximum size of a single log file.                                                                                                                                            	|             	
| `KAFKA_log_retention_ms`              	| `` | 1.8.0 	| The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in `log.retention.hours` is used. If set to `-1`, no time limit is applied.                                                                                                                                            	|
| `KAFKA_log_retention_hours`              	| `168` (1 GB) | 1.8.0 	| The number of hours to keep a log file before deleting it (in hours), tertiary to `log.retention.ms` property                                                                                                                                            	|
| `KAFKA_log_retention_bytes`              	| `-1` (not used) | 1.8.0 	| The maximum size of the log before deleting it.                                                                                                                                            	|
| `KAFKA_compression_type`              	| `producer` | 1.8.0 	| Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `uncompressed` which is equivalent to no compression; and `producer` which means retain the original compression codec set by the producer.                                                                                                                                            	|
| `KAFKA_min_insync_replicas`              	| `1`  | 1.8.0 	| When a producer sets acks to "all" (or "-1"), `min.insync.replicas` specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).                                                                                                                                  	|             	
| `KAFKA_replica_selector_class`              	|  	| 1.8.0 	| The fully qualified class name that implements ReplicaSelector. This is used by the broker to find the preferred read replica. By default, an implementation that returns the leader.                                                                                                                                            	|             	|                                	|
| `KAFKA_confluent_log_placement_constraints`              	| `{}` 	| 1.8.0 	| This configuration is a JSON object that controls the set of brokers (replicas) which will always be allowed to join the ISR. And the set of brokers (observers) which are not allowed to join the ISR. Only enabled if `KAFKA_edition` is set to `enterprise`. Find more information here: <https://docs.confluent.io/current/installation/configuration/broker-configs.html>.                                                                                                                                             	|             	|                                	|
| `KAFKA_confluent_tier_feature`              	| `false` 	| 1.8.0 	| enables Tiered Storage for a broker. Setting this to `true` allows a broker to utilize Tiered Storage                                                                                                                                             	|  
| `KAFKA_confluent_tier_enable`              	| `false` 	| 1.8.0 	| sets the default value for created topics. Setting this to `true` causes all non-compacted topics to be tiered                                                                                                                                             	|  
| `KAFKA_confluent_tier_backend`              	| `S3` 	| 1.8.0 	| refers to the cloud storage service to which a broker will connect, either `S3` for Amazon S3 or `GCS` for Google Cloud Storage.                                                                                                                                             	|  
| `KAFKA_confluent_tier_s3_bucket`              	| `kafka-logs` 	| 1.8.0 	| the S3 bucket name used for writing and reading tiered data                                                                                                                                             	|  
| `KAFKA_confluent_tier_s3_prefix`              	| `` 	| 1.8.0 	| This prefix will be added to tiered storage objects stored in S3.                                                                                                                                                  	|  
| `KAFKA_confluent_tier_s3_region`              	| `us-east-1` 	| 1.8.0 	| the S3 region used for writing and reading tiered data                                                                                                                                                  	|
| `KAFKA_confluent_tier_s3_aws_endpoint_override`              	| `` 	| 1.8.0 	| override the endpoint of the S3 storage, if an on-premises storage such as Minio is used.                                                                                                                                                  	|
| `KAFKA_confluent_tier_s3_force_path_style_access`              	| `false` 	| 1.13.0 	| Configures the client to use path-style access for all requests. This flag is not enabled by default. The default behavior is to detect which access style to use based on the configured endpoint and the bucket being accessed. Setting this flag will result in path-style access being forced for all requests.                                                                                                                                                  	|
| `KAFKA_confluent_tier_local_hotset_bytes`              	| `-1` 	| 1.8.0 	| When tiering is enabled, this configuration controls the maximum size a partition (which consists of log segments) can grow to on broker-local storage before we will discard old log segments to free up space. Log segments retained on broker-local storage is referred as the "hotset". Segments discarded from local store could continue to exist in tiered storage and remain available for fetches depending on retention configurations. By default there is no size limit only a time limit. Since this limit is enforced at the partition level, multiply it by the number of partitions to compute the topic hotset in bytes.                                                                                                                                                  	|
| `KAFKA_confluent_tier_local_hotset_ms`              	| `86400000` (1 day)	| 1.8.0 	| When tiering is enabled, this configuration controls the maximum time we will retain a log segment on broker-local storage before we will discard it to free up space. Segments discarded from local store could continue to exist in tiered storage and remain available for fetches depending on retention configurations. If set to -1, no time limit is applied.                                                                                                                                                	|
| `KAFKA_confluent_tier_archiver_num_threads`              	| `2` 	| 1.8.0 	| The size of the thread pool used for tiering data to remote storage. This thread pool is also used to garbage collect data in tiered storage that has been deleted.                                                                                                                                                  	|
| `KAFKA_confluent_tier_fetcher_num_threads`              	| `4` 	| 1.8.0 	| The size of the thread pool used by the TierFetcher. Roughly corresponds to number of concurrent fetch requests that can be served from tiered storage.                                                                                                                                                  	|
| `KAFKA_confluent_tier_topic_delete_check_interval_ms`              	| `10800000` (3 hours) 	| 1.8.0 	| Frequency at which tiered objects cleanup is run for deleted topics.                                                                                                                                                 	|
| `KAFKA_confluent_tier_metadata_replication_factor`              	| `1` 	| 1.8.0 	| The replication factor for the tier metadata topic (set higher to ensure availability).                                                                                                                                                  	|
| `KAFKA_log4j_root_level`              	| `INFO` 	| 1.14.0 	| Change the default log4j logging levels of Kafka broker.                                                                                                                                                  	|
| `KAFKA_log4j_loggers` | `` 	| 1.14.0 	| Add new logging levels for a Confluent Platform component, i.e. to override the log levels for Kafka controller and request loggers use `kafka.controller=TRACE,kafka.request.logger=WARN`.                                                                                                                                             	|
| `KAFKA_tools_log4j_level`              	| `INFO` 	| 1.14.0 	| Change the default log4j logging levels of the Kafka tools.                                                                                                                                                  	|
| [**_Kafka CLI_**](./services/kafka-cli)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                    	|             	|                               
| `KAFKA_CLI_enable`                                 	| `false` 	| 1.16.0 	| Use confluent Kafka                                                                                                                                                                 	|             	|                                	|
| **_Schema Registry_**   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                    	|             	|                                	|
| `SCHEMA_REGISTRY_enable`                 	| `false` 	| 1.14.0 	| Generate Confluent Schema Registry service                                                                                                                                          	|             	|                                	|
| `SCHEMA_REGISTRY_flavour`                  	| `confluent` 	| 1.14.0 	| Which schema registry to use, either `confluent` or `apicurio`                                                                                                                                           	|             	|                                	|
| `SCHEMA_REGISTRY_nodes`                  	| `false` 	| 1.14.0 	| number of Confluent Schema Registry nodes                                                                                                                                           	|             	|                                	|
| - [**_Confluent Schema Registry_**](./services/schema-registry)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                    	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_enable`                 	| `false` 	| 1.14.0 	| Generate Confluent Schema Registry service - **Deprecated!!** will just set the `SCHEMA_REGISTRY_enable` and the `SCHEMA_REGISTRY_flavour` to `confluent`.                                                                                                                                         	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_use_zookeeper_election` 	| `false` 	| 1.14.0 	| use Zookeeper for election of "master" Schema Registry node                                                                                                                         	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_replication_factor`     	|   `1`   	| 1.14.0 	| replication factor to use for the `_schemas` topic                                                                                                                                  	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_leader_eligibility`     	|   `true`   	| 1.14.0 	| if `true`, this node can participate in primary election. In a multi-colocated setup, turn this off for clusters in the secondary data center.                                                                                                                                  	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_mode_mutability`     	|   `true`   	| 1.14.0 	| if `true` the mode of this Schema Registry node can be changed.                                                                                                                                  	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_schema_compatibility_level`     	|   `backward`   	| 1.14.0 	| The schema compatibility type. One of `none`, `backward`, `backward_transitive`, `forward`, `forward_transitive`, `full` or `full_transitive`.                                                                                                                               	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_log4j_root_loglevel`     	|   `info`   	| 1.14.0 	| Change the `rootLogger` loglevel of the Schema Registry.                                                                                                                              	|             	|                                	|
| `CONFLUENT_SCHEMA_REGISTRY_debug`     	|   `false`   	| 1.14.0 	| Boolean indicating whether extra debugging information is generated in some error response entities.                                                                                                                               	|             	|                                	|
| - [**_Apicurio Registry_**](./services/apicurio-registry)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `APICURIO_SCHEMA_REGISTRY_storage`                  	| `kafkasql` 	| 1.14.0 	| The storage type to use, either `mem` for In-Memory, `sql` for Postgresql, 'mssql' for SQL Server or `kafkasql` for Kafka based storage.                                                                                                                                      	|             	|                                	|
| `APICURIO_SCHEMA_REGISTRY_sql_storage_database`                  	| `apicuriodb` 	| 1.14.0 	| The database to use if storage is `sql` or `mssql`.                                                                                                                                      	|             	|                                	|
| `APICURIO_SCHEMA_REGISTRY_sql_storage_user`                  	| `apicurio` 	| 1.14.0 	| The user to use if storage is `sql` or `mssql`.                                                                                                                                   	|             	|                                	|
| `APICURIO_SCHEMA_REGISTRY_sql_storage_password`                  	| `abc123!` 	| 1.14.0 	| The password to use if storage is `sql` or `mssql`.                                                                                                                                     	|             	|                                	|
| `APICURIO_auth_enabled`                  	| `false` 	| 1.14.0 	| Enable authentication using Keycloak? If set to `true` then `KEYCLOAK_enable` will be enabled automatically.                                                                                                                                     	|             	|                                	|
| `APICURIO_auth_anonymous_read_access_enabled`                  	| `false` 	| 1.14.0 	| Allow anonymous users (REST API calls with no authentication credentials provided) to make read-only calls to the REST API.                                                                                                                                     	|             	|                                	|
| `APICURIO_auth_import_default_users`                  	| `false` 	| 1.14.0 	| If set to `true`, imports the following pre-defined users `sr-view`, `sr-dev` and `sr-admin` into the `registry` realm.                                                                                                                                      	|             	|                                	|
| `APICURIO_basic_auth_enabled`                  	| `false` 	| 1.14.0 	| Enable basic authentication? To set it to `true`, `APICURIO_auth_enable` has to be enabled as well and Keycloak configured accordingly. Basic authentication is just the authentication API layer, the authentication will still happen over keycloak.                                                                                                                                    	|             	|                                	|
| `APICURIO_eventsourcing_enabled`                  	| `false` 	| 1.14.0 	| Enable Event Sourcing, i.e. enable the Schema Registry to send events when changes are made to the registry.                                                                                                                                     	|             	|                                	|
| `APICURIO_eventsourcing_transport`                  	| `kafka` 	| 1.14.0 	| The Protocol to use for transporting the events. Either `kafka` or `http` are supported. The events are formatted using the CNCF Cloud Events specification.                                                                                                                                    	|             	|                                	|
| `APICURIO_eventsourcing_kafka_topic`                  	| `registry-events` 	| 1.14.0 	| The Kafka topic to use, if `APICURIO_eventsourcing_transport` is set to `kafka`.                                                                                                                                    	|             	|                                	|
| `APICURIO_eventsourcing_http_endpoint`                  	| `registry-events` 	| 1.14.0 	| The HTTP endpoint to push the data to, if `APICURIO_eventsourcing_transport` is set to `http`.                                                                                                                                    	|             	|                                	|
| [**_Apache Kafka Connect_**](./services/kafka-connect)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                     	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKA_CONNECT_enable`                         	| `false` 	| 1.2.0 	| Generate Kafka Connect service                                                                                                                                                      	|             	|                                	|
| `KAFKA_CONNECT_nodes`                          	|   `1`   	| 1.13.0 	| number of Kafka Connect nodes                                                                                                                                                       	|             	|                                	|
| `KAFKA_CONNECT_connectors`                          	|      	| 1.6.0 	| A comma separated list of components to be installed from [Confluent Hub](https://www.confluent.io/hub). Specify identifier of the form owner/component:version for the component in Confluent Hub.                                                                                                                                                 	|  
| `KAFKA_CONNECT_config_providers`                          	|      	| 1.9.0 	|  A comma-separated list of names for [ConfigProviders](https://docs.confluent.io/home/connect/userguide.html#configprovider-interface). Allows to use variables in connector configurations that are dynamically resolved when the connector is (re)started. Can be used for secrets or any other configuration information which should be resolved dynamically at runtime.                                                                                                                                              	|   
| `KAFKA_CONNECT_config_providers_classes`                          	|      	| 1.9.0 	|  The Java class names for the providers listed in the `KAFKA_CONNECT_config_providers` property.                                                                                                                                              	|   
| `KAFKA_CONNECT_map_settings_file`                          	|  `false`    	| 1.9.0 	|  Map the `settings.properties` file placed in the `$DATAPLATFORM_HOME/conf.override/kafka-connect/` folder into the container. Use it when enabling the `FileConfigProvider` trough the `KAFKA_CONNECT_config_providers` and `KAFKA_CONNECT_config_providers_classes` properties.                                                                                                                                              	|                 	
| [**_ksqlDB_**](./services/ksqldb)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                  	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKA_KSQLDB_enable`                          	| `false` 	| 1.2.0 	| ksqlDB is streaming SQL on Kafka. If you enable it, then SCHEMA_REGISTRY_enable will be automatically set to `true`.                                                                                                                                                             	|             	|                                	|
| `KAFKA_KSQLDB_edition`                          	| `oss` 	| 1.15.0 	| the edition to use, either `oss` for the Open Source or `cp` for the Confluent Platform.                                                                                                                                                             	|             	|                                	|
| `KAFKA_KSQLDB_nodes`                           	|   `2`   	| 1.2.0 	| number of ksqlDB nodes                                                                                                                                                              	|             	|                                	|
| `KAFKA_KSQLDB_suppress_enabled`                           	|   `false`   	| 1.9.0 	| Enable the Suppress functionality which have been added with ksqldb 0.13.0                                                                                                                                                            	|     
| `KAFKA_KSQLDB_suppress_buffer_size_bytes`                           	|   ``   	| 1.9.0 	| Bound the number of bytes that the buffer can use for suppression. Negative size means the buffer will be unbounded. If the maximum capacity is exceeded, the query will be terminated.                                                                                                                                                            	|           	
| `KAFKA_KSQLDB_query_pull_table_scan_enabled`                           	|   `false`   	| 1.12.0 	| Config to control whether table scans are permitted when executing pull queries. Works with ksqlDB > 0.17.0.                                                                                                                                                            	|  
| `KAFKA_KSQLDB_response_http_headers_config`                           	|   ``   	| 1.12.0 	| Use to select which HTTP headers are returned in the HTTP response for Confluent Platform components. Specify multiple values in a comma-separated string using the format `[action][header name]:[header value]` where `[action]` is one of the following: `set`, `add`, `setDate`, or `addDate`.                                                                                                                                                           	|           	
| `KAFKA_KSQLDB_queries_file`                           	|   ``   	| 1.9.0 	| A file that specifies a predefined set of queries for the ksqlDB cluster.                                                                                                                                                            	|     
| `KAFKA_KSQLDB_use_embedded_connect`                           	|   `false`   	| 1.13.0 	| Enable embedded kafka connect. Place connector jars into `./plugins/kafka-connect` or install them from [Confluent Hub](https://www.confluent.io/hub) using the `KAFKA_KSQLDB_connect_connectors` property. **Important:** Will only be effective, if `KAFKA_CONNECT_enable` is set to `false`.                                                                                                                                                           	|
| `KAFKA_KSQLDB_connect_connectors`                           	|   ``   	| 1.13.0 	| A comma separated list of components to be installed from [Confluent Hub](https://www.confluent.io/hub). Specify identifier of the form owner/component:version for the component in Confluent Hub.                                                                                                                                                        	|      
| `KAFKA_KSQLDB_persistence_default_format_key`                           	|   `KAFKA`   	| 1.15.0 	| Sets the default value for the KEY_FORMAT property if one is not supplied explicitly in CREATE TABLE or CREATE STREAM statements.                                                                                                                                                        	|      
| `KAFKA_KSQLDB_persistence_default_format_value`                           	|   ``   	| 1.15.0 	| Sets the default value for the VALUE_FORMAT property if one is not supplied explicitly in CREATE TABLE or CREATE STREAM statements.                                                                                                                                                       	|      
| `KAFKA_KSQLDB_log_topic`                           	|   `ksql_processing_log`   	| 1.16.0 	| the name of the processing log Kafka topic.                                                                                                                                                       	|      
| [**_Materialize_**](./services/materialize)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                  	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `MATERIALIZE_enable`                          	| `false` 	| 1.12.0 	| Enable Materialize streaming database for real-time applications.                                                                                                                                                              	|             	|                                	|
| [**_Azkarra Streams_**](./services/azkarra-worker)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                  	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `AZKARRA_enable`                          	| `false` 	| 1.8.0 	| Enable Azkarra Streams Worker, a lightweight Java framework which makes easy to develop and operate Kafka Streams applications.                                                                                                                                                              	|             	|                                	|
| [**_Confluent Replicator_**](./services/confluent-replicator)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                  	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKA_REPLICATOR_enable`                          	| `false` 	| 1.6.0 	| Enable Confluent Replicator (part of Confluent Enterprise Platform).                                                                                                                                                             	|             	|                                	|
| [**_Kafka Mirror Maker 2_**](./services/mirror-maker2)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                  	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKA_MM2_enable`                          	| `false` 	| 1.14.0 	| Enable Kafka Mirror Maker 2.                                                                                                                                                             	|             	|                                	|
| [**_Confluent REST Proxy_**](./services/kafka-rest)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKA_RESTPROXY_enable`                       	| `false` 	| 1.2.0 	| Generate Confluent REST Proxy service                                                                                                                                               	|             	|                                	|
| [**_Confluent MQTT Proxy_**](./services/kafka-mqtt)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKA_MQTTPROXY_enable`                       	| `false` 	| 1.2.0 	| Generate Confluent MQTT Proxy service                                                                                                                                               	|             	|                                	|
| `KAFKA_MQTTPROXY_topic_regex_list`                       	| `` 	| 1.8.0 	| A comma-separated list of pairs of type <kafka topic>:<regex> that is used to map MQTT topics to Kafka topics.                                                                                                                                               	|             	|                                	|
| [**_Zilla_**](./services/zilla)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)            	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `ZILLA_enable`                       	| `false` 	| 1.15.0 	| Generate Zilla service                                                                                                                                               	|             	|                                	|
| [**_Lenses Box_**](./services/lenses-box)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `LENSES_BOX_enable`              	| `false` 	| 1.14.0 	| Generate Lenses Box (Development) service.                                                                                                                                     	|             	|                                	|
| `LENSES_BOX_license`              	| `false` 	| 1.14.0 	| Set the end-user-license string you have gotten from <http://lenses.io> by email.                                                                                                                                     	|             	|                                	|
| [**_kcat (used to be kafkacat)_**](./services/kcat)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KCAT_enable`              	| `false` 	| 1.13.0 	| Generate `kcat` CLI service                                                                                                                                         	|             	|                                	|
| [**_kaskade_**](./services/kaskade)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KASKADE_enable`              	| `false` 	| 1.16.0 	| Generate `kaskade` CLI service                                                                                                                                         	|             	|                                	|
| [**_kafkactl_**](./services/kafkactl)   &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                	|         	|       	|                                                                                                                                                                                     	|             	|                                	|
| `KAFKACTL_enable`              	| `false` 	| 1.15.0 	| Generate `kafkactl` CLI service                                                                                                                                         	|             	|                                
| [**_jikkou_**](./services/jikkou)  &nbsp;&nbsp;&nbsp;&nbsp;![x86-64](./images/x86-64.png)                                	|         	|       	|                                                                                                                                                                                     	|             	|                                	|