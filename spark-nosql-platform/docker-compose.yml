# =======================================================================
# Platform Name            spark-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  develop
# =======================================================================
networks:
  default:
    name: spark-platform
services:
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: bitnami/spark:3.5.2
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.name: spark
      com.platys.description: Spark Master Node
      com.platys.webui.title: Spark UI
      com.platys.webui.url: http://dataplatform:28304
    ports:
      - 28304:28304
      - 6066:6066
      - 7077:7077
      - 4040-4044:4040-4044
    env_file:
      - ./conf/hadoop.env
    environment:
      # bitnami env vars
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_MASTER_WEBUI_PORT: 28304
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
#      INIT_DAEMON_STEP: setup_spark
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir:
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: ''
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-core_2.12:3.2.0,io.delta:delta-storage:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
#      - ./scripts/docker/maven-download.sh:/usr/src/app/maven-download.sh
      - spark-vol:/opt/bitnami/spark
    restart: unless-stopped
  spark-worker-1:
    image: bitnami/spark:3.5.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    labels:
      com.platys.name: spark
      com.platys.description: Spark Worker Node
    depends_on:
      - spark-master
    ports:
      - 28111:28111
    env_file:
      - ./conf/hadoop.env
    environment:
      # bitnami env vars
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_WORKER_WEBUI_PORT: '28111'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir:
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: ''
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-core_2.12:3.2.0,io.delta:delta-storage:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-2:
    image: bitnami/spark:3.5.2
    container_name: spark-worker-2
    hostname: spark-worker-2
    labels:
      com.platys.name: spark
      com.platys.description: Spark Worker Node
    depends_on:
      - spark-master
    ports:
      - 28112:28112
    env_file:
      - ./conf/hadoop.env
    environment:
      # bitnami env vars
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_WORKER_WEBUI_PORT: '28112'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir:
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: ''
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-core_2.12:3.2.0,io.delta:delta-storage:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-history:
    image: bitnami/spark:3.5.2
    container_name: spark-history
    hostname: spark-history
    labels:
      com.platys.name: spark-historyserver
      com.platys.description: Spark History Server
      com.platys.webui.title: Spark History Server
      com.platys.webui.url: http://dataplatform:28117
      com.platys.restapi.title: Spark History Server
      com.platys.restapi.url: http://dataplatform:28117/api/v1
    expose:
      - 18080
    ports:
      - 28117:18080
    environment:
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: ''
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-core_2.12:3.2.0,io.delta:delta-storage:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir:
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    restart: unless-stopped
  spark-thriftserver:
    image: bitnami/spark:3.5.2
    container_name: spark-thriftserver
    hostname: spark-thriftserver
    labels:
      com.platys.name: spark-thriftserver
      com.platys.description: Spark Thriftserver
      com.platys.webui.title: Spark Thriftserver UI
      com.platys.webui.url: http://dataplatform:28298
    ports:
      - 28118:10000
      - 28298:4040
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir:
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_cores_max: 4
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: ''
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-core_2.12:3.2.0,io.delta:delta-storage:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    command:
      - bash
      - -c
      - |
        /usr/src/app/wait-for-it.sh hive-metastore:9083 --timeout=120 -- echo "now starting spark thriftserver ...."
        /opt/bitnami/spark/sbin/start-thriftserver.sh --master spark://spark-master:7077
    restart: unless-stopped
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:3.1.2-postgresql-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore
    ports:
      - 9083:9083
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      # necessary for Trino to be able to read from Avro
      HIVE_SITE_CONF_metastore_storage_schema_reader_impl: org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader
      SERVICE_PRECONDITION: hive-metastore-db:5432
    volumes:
      - ./data-transfer:/data-transfer
    command: /opt/hive/bin/hive --service metastore
    restart: unless-stopped
  hive-metastore-db:
    image: trivadis/apache-hive-metastore-postgresql:3.1.0-postgres9.5.3
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore DB
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Jupyter ========================================== #
  jupyter:
    image: quay.io/jupyter/all-spark-notebook:spark-3.5.2
    container_name: jupyter
    hostname: jupyter
    labels:
      com.platys.name: jupyter
      com.platys.description: Web-based interactive development environment for notebooks, code, and data
      com.platys.webui.title: Jupyter UI
      com.platys.webui.url: http://dataplatform:28888
    ports:
      - 28888:8888
      - 28376-28380:4040-4044
    user: root
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      JUPYTER_ENABLE_LAB: "'yes'"
      GRANT_SUDO: "'yes'"
      JUPYTER_TOKEN: abc123!
      DOCKER_STACKS_JUPYTER_CMD: lab
      MAVEN_DOWNLOAD_JARS: com.amazonaws:aws-java-sdk-bundle:1.11.375,org.apache.hadoop:hadoop-aws:3.2.1,com.google.guava:guava:14.0.1
      # remove some JARS if they are conflicting with the ones installed above
      REMOVE_JARS: guava-14.0.1.jar
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/jupyter/jupyter-init.sh:/usr/local/bin/start-notebook.d/jupyter-init.sh
      - ./scripts/docker/maven-download.sh:/maven-download.sh
    #  - "./conf/jupyter/spark-defaults.conf:/usr/local/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf"
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        start-notebook.sh
    restart: unless-stopped
  #  ================================== Cassandra ========================================== #
  cassandra-1:
    image: bitnami/cassandra:4.1
    container_name: cassandra-1
    hostname: cassandra-1
    labels:
      com.platys.name: cassandra
      com.platys.description: wide-column NoSQL database
    ports:
      - 29042:9042
      - 7199:7199
      - 9160:9160
    environment:
      - CASSANDRA_CLUSTER_NAME="Test Cluster"
      - CASSANDRA_DATACENTER=se1
      - CASSANDRA_PASSWORD_SEEDER=yes
      - CASSANDRA_USER=cassandra
      - CASSANDRA_PASSWORD=cassandra
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_NUM_TOKENS=128
      - LOCAL_JMX=yes
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/cassandra/jmxremote.access:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management/jmxremote.access
      - ./conf/cassandra/jmxremote.password:/etc/cassandra/jmxremote.password
    restart: unless-stopped
  #  ================================== InfluxDB 2 ========================================== #
  influxdb2:
    image: influxdb:2.7
    hostname: influxdb2
    container_name: influxdb2
    labels:
      com.platys.name: influxdb2
      com.platys.description: Timeseries Database
      com.platys.webui.title: InfluxDB 2.0 UI
      com.platys.webui.url: http://dataplatform:19999
      com.platys.restapi.title: InfluxDB 2.0 Rest API
      com.platys.restapi.url: http://dataplatform:19999/api/v2
    ports:
      - 19999:8086
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: influx
      DOCKER_INFLUXDB_INIT_PASSWORD: abc123abc123!
      DOCKER_INFLUXDB_INIT_ORG: demo
      DOCKER_INFLUXDB_INIT_BUCKET: demo-bucket
      DOCKER_INFLUXDB_INIT_RETENTION: 1w
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/influxdb2:/docker-entrypoint-initdb.d
    command: --reporting-disabled
    restart: unless-stopped
  #  ================================== Minio ========================================== #
  minio-1:
    image: minio/minio:RELEASE.2024-04-06T05-26-02Z
    container_name: minio-1
    hostname: minio-1
    labels:
      com.platys.name: minio
      com.platys.description: Software-defined Object Storage
      com.platys.webui.title: MinIO UI
      com.platys.webui.url: http://dataplatform:9000
    ports:
      - 9000:9000
      - 9010:9010
    environment:
      MINIO_ROOT_USER: V42FCGRVMK24JJ8DHUYG
      MINIO_ROOT_PASSWORD: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      MINIO_REGION_NAME: us-east-1
      MINIO_DOMAIN: minio
      MINIO_SERVER_URL: http://${PUBLIC_IP}:9000
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus-1:9090
    volumes:
      - ./data-transfer:/data-transfer
    command: server /data --console-address ":9010"
    restart: unless-stopped
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/9000; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: minio
      com.platys.description: MinIO Console
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    entrypoint:
      - /bin/sh
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 minio-1:9000
        mc alias set minio-1 http://minio-1:9000 V42FCGRVMK24JJ8DHUYG bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
        mc mb --ignore-existing minio-1/admin-bucket
           for bucket in $$(tr ',' '\n' <<< "")
        do
          mc mb --ignore-existing minio-1/$$bucket
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: dannyben/madness:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.description: Platys Platform homepage viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://dataplatform:80
    ports:
      - 80:3000
    volumes:
      - ./artefacts:/docs
      - ./conf/markdown-viewer/markdown-madness.yml:/docs/.madness.yml
      - ./data-transfer:/data-transfer
    command: server
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, -f, http://dataplatform:80]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 1m
  markdown-renderer:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
      com.platys.description: Platys Platform homepage rendering
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: spark-platform
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: develop
      PLATYS_COPY_COOKBOOK_DATA: 'True'
      SERVICE_LIST_VERSION: 2
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
    init: true
  #  ================================== data-provisioning ========================================== #
  data-provisioning:
    image: trivadis/platys-modern-data-platform-data:latest
    container_name: data-provisioning
    hostname: data-provisioning
    labels:
      com.platys.name: data-provisioning
      com.platys.description: Provisioning sample data
    volumes:
      - ./data-transfer:/data-transfer
    init: true
volumes:
  data-transfer-vol:
    name: data_transfer_vol
  spark-vol:
